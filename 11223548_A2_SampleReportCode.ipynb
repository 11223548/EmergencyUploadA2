{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "11223548_A2_SampleReportCode.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/11223548/EmergencyUploadA2/blob/master/11223548_A2_SampleReportCode.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3K_0PgZQIDR",
        "colab_type": "text"
      },
      "source": [
        "#Assignment 2: Simplified Report Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPk5boqzQTcf",
        "colab_type": "text"
      },
      "source": [
        "## Important Disclaimer\n",
        "<br>As discussed in the assignment 2 report, the dataset used in the report was subject to strict prohibitions against redistribution as it is a proprietary dataset. As a result, the code here loads an anonymised sub-sample of the dataset that has removed many of the identifying features of the data. Furthermore, since the data preprocessing took multiple days on a computing cluster it is not practical to include that section of the script here. Instead this script will upload a pre-processed, anonymised sub-sample of the merged proprietary dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVx-x7nQRD6k",
        "colab_type": "text"
      },
      "source": [
        "#Simplified Report Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L__Cz4LxRN7P",
        "colab_type": "text"
      },
      "source": [
        "***Import of Python Libraries***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRrt5ffiRTME",
        "colab_type": "code",
        "outputId": "70c95789-16d6-4257-a262-10d1f95c7c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import random\n",
        "import seaborn as sns \n",
        "import matplotlib.pyplot as plt \n",
        "from matplotlib import cm as cm\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "import time as tm \n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn.over_sampling import RandomOverSampler \n",
        "from imblearn.over_sampling import SMOTE\n",
        "import gspread\n",
        "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tg6jwzZ0RvN0",
        "colab_type": "text"
      },
      "source": [
        "***Import of Pre-Processed Data***\n",
        "<br>\n",
        "<br>Full_Sample represents the data for Implementation 1 discussed in the report.\n",
        "<br>Full_Sample_M2 represents the data for Implementation 2 discussed in the report.\n",
        "\n",
        "<br>By default \"Full_Sample\" is the data will flow through to subsequent sections of this script. In order to run the second implementation through the rest of the model, simply uncomment the final low of code below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smqM7KPVt_fK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "772f63c2-e6ea-4285-d0fa-2a58f41b660b"
      },
      "source": [
        "# Fetch Implementation 1 Data from GitHub.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://github.com/11223548/UTS_ML2019_Main/blob/master/_M1.csv"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  335k    0  335k    0     0   538k      0 --:--:-- --:--:-- --:--:--  537k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHh3pCIFuY3c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3e3bd456-417c-4330-eb6b-047ac8254907"
      },
      "source": [
        "# Fetch Implementation 2 Data from GitHub.\n",
        "!curl --remote-name \\\n",
        "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
        "     --location https://github.com/11223548/UTS_ML2019_Main/blob/master/_M2.csv"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  514k    0  514k    0     0  1180k      0 --:--:-- --:--:-- --:--:-- 1177k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE7T2z3ZoGZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "URL_1 = \"https://github.com/11223548/UTS_ML2019_Main/blob/master/_M1.csv\"\n",
        "URL_2 = \"https://github.com/11223548/UTS_ML2019_Main/blob/master/_M2.csv\"\n",
        "\n",
        "Full_Sample = pd.read_csv(URL_1) # Model Implementation 1 Data\n",
        "Full_Sample_M2 = pd.read_csv(URL_2) # Model Implementation 2 Data\n",
        "\n",
        "#Full_Sample=Full_Sample_M2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmMDt0X3nFoF",
        "colab_type": "text"
      },
      "source": [
        "***Defining Some Functions used Throughout Script***\n",
        "<br>\n",
        "1. A progress bar for loops \n",
        "<br>*(this was more important for data pre-processing where scripts would run for many hours - not soo much here)*\n",
        "2. A correlation matrix function\n",
        "3. An F1-Score function\n",
        "4. A confusion matrix plot function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD_sK_5NnF_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print iterations progress\n",
        "def printProgressBar (iteration, total, prefix = '', suffix = '', decimals = 1, length = 100, fill = 'â–ˆ'):\n",
        "    \"\"\"\n",
        "    Call in a loop to create terminal progress bar\n",
        "    @params:\n",
        "        iteration   - Required  : current iteration (Int)\n",
        "        total       - Required  : total iterations (Int)\n",
        "        prefix      - Optional  : prefix string (Str)\n",
        "        suffix      - Optional  : suffix string (Str)\n",
        "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
        "        length      - Optional  : character length of bar (Int)\n",
        "        fill        - Optional  : bar fill character (Str)\n",
        "    \"\"\"\n",
        "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
        "    filledLength = int(length * iteration // total)\n",
        "    bar = fill * filledLength + '-' * (length - filledLength)\n",
        "    print('\\r%s |%s| %s%% %s' % (prefix, bar, percent, suffix), end = '\\r')\n",
        "    # Print New Line on Complete\n",
        "    if iteration == total: \n",
        "        print()\n",
        "\n",
        "\n",
        "def correlation_matrix(df,labels):\n",
        "    fig = plt.figure()\n",
        "    ax1 = fig.add_subplot(111)\n",
        "    cmap = cm.get_cmap('jet', 30)\n",
        "    cax = ax1.imshow(df.corr(), interpolation=\"nearest\", cmap=cmap)\n",
        "    ax1.grid(True)\n",
        "    plt.title('Correlation Matrix', fontsize = 40)\n",
        "    #labels = [\"age\", \"duration\", \"campaign\", \"previous\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\", \"nr.employed\", \"BinFinalY\"]\n",
        "    ax1.set_xticks(np.arange(len(labels)))\n",
        "    ax1.set_yticks(np.arange(len(labels)))\n",
        "    ax1.set_xticklabels(labels,fontsize=20, rotation=90)\n",
        "    ax1.set_yticklabels(labels,fontsize=20)\n",
        "    # Add colorbar, make sure to specify tick locations to match desired ticklabels\n",
        "    fig.colorbar(cax, ticks=[-1,-0.75,-0.5,-.25,.0,.25,.50,.75,1])\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    \n",
        "def F1_Score(ConfusionMatrix):\n",
        "    True_Pos = ConfusionMatrix[1,1] \n",
        "    False_Pos = ConfusionMatrix[0,1] \n",
        "    #True_Neg = ConfusionMatrix[0,0]\n",
        "    False_Neg = ConfusionMatrix[1,0]\n",
        "    Precision = True_Pos / (True_Pos + False_Pos)\n",
        "    Recall = True_Pos / (True_Pos + False_Neg)\n",
        "    F_score = 2 * (Precision * Recall) / (Precision + Recall)\n",
        "    return F_score\n",
        "  \n",
        "  \n",
        "\n",
        "def plot_confusion_matrix(y_true, y_pred, \n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    # Only use the labels that appear in the data\n",
        "    classes = [\"No Insider Trading\", \"Insider Trading\"]\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    # We want to show all ticks...\n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           # ... and label them with the respective list entries\n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    # Rotate the tick labels and set their alignment.\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    # Loop over data dimensions and create text annotations.\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIEpNI-KTERL",
        "colab_type": "text"
      },
      "source": [
        "***Prospect Data for Likely Discriminative Features***\n",
        "<br>\n",
        "<br> The below code plots the conditional distributions of input variables to the neural networks as well as a correlation matrix. These are used prior to analysis to provide quick visual insight into variables that are likely to be effective inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkT8TXT4TJYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Some Filters for Plotting\n",
        "Deltas = [\"Delta_Price\",\"Delta_Avg_Vol.\",\"Delta_Tot_Vol.\"]\n",
        "D_IT = Deltas + [\"IT_Flag\"]\n",
        "\n",
        "sns.pairplot(Imp1_Data[D_IT], hue=\"IT_Flag\") #distinguishes each scatter plot by data points associated with \"yes\" (subscription) and \"no\" (no subscription)\n",
        "\n",
        "correlation_matrix(Full_Sample[D_IT],CorMatFilter)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMjWRgnDob2X",
        "colab_type": "text"
      },
      "source": [
        "***Assign Observations to Training/Cross-Validation/Testing***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FlYldLZo_OK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Identify remaining unique IT prosecuted cases \n",
        "UniqueTickers = pd.DataFrame(Full_Sample[\"Ticker\"].copy().unique().tolist(),columns=[\"Tickers\"])\n",
        "UniqueTickers[\"IT_Count\"]=0\n",
        "\n",
        "for row in range(len(UniqueTickers)):\n",
        "    TempTick = UniqueTickers[\"Tickers\"].iloc[row]\n",
        "    Tick_IT_Count = Full_Sample[\"IT_Flag\"][(Full_Sample[\"Ticker\"]==TempTick)].sum()\n",
        "    UniqueTickers[\"IT_Count\"].iloc[row] = Tick_IT_Count\n",
        "\n",
        "UniqueTickers = UniqueTickers[(UniqueTickers[\"IT_Count\"]>0)]\n",
        "UniqueTickers = UniqueTickers.iloc[:,0].tolist()\n",
        "\n",
        "# Assign data to training, cross-validation or testing samples\n",
        "Imports = len(UniqueTickers)\n",
        "Imp_Increment1 = int(round(0.6*Imports,0))\n",
        "Imp_Increment2 = int(round(0.8*Imports,0))\n",
        "Train_Tickers =  pd.DataFrame(UniqueTickers[0:Imp_Increment1].copy(),columns=[\"Tickers\"]) # allocate 60% of imports to training data\n",
        "CV_Tickers = pd.DataFrame(UniqueTickers[Imp_Increment1:Imp_Increment2].copy(),columns=[\"Tickers\"]) # allocate 20% of imports to cross-validation data\n",
        "Test_Tickers = pd.DataFrame(UniqueTickers[Imp_Increment2:].copy(),columns=[\"Tickers\"]) # allocate 20% of imports to cross-validation data\n",
        "\n",
        "# Apply filters to full_sample to split into respective sets\n",
        "Train_Mask = Full_Sample[\"Ticker\"].isin(Train_Tickers[\"Tickers\"])\n",
        "FinParameters_All_Train = Full_Sample[Train_Mask].copy()\n",
        "\n",
        "CV_Mask = Full_Sample[\"Ticker\"].isin(CV_Tickers[\"Tickers\"])\n",
        "FinParameters_All_CV = Full_Sample[CV_Mask].copy()\n",
        "\n",
        "Test_Mask = Full_Sample[\"Ticker\"].isin(Test_Tickers[\"Tickers\"])\n",
        "FinParameters_All_Test = Full_Sample[Test_Mask].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWZ4rirfp_jr",
        "colab_type": "text"
      },
      "source": [
        "***OverSampling of Minority Class***\n",
        "<br>Here the default has been set to random oversampling with SMOTE commented out. This can be quickly reversed by commenting out random oversampling and uncommenting SMOTE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NpHsi9v6qAR4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%% Perform Partial Random Oversampling with replacement of Minority Class (training set only --> exclude cross val and test set)\n",
        "\n",
        "MinClassPortion = 1 # 1 = balanced 50/50 between minority/majority class\n",
        "\n",
        "#Isolate target variable from rest of dataset\n",
        "X_train_num = np.array(FinParameters_All_Train[ParamColumnHeaders[2:]]) #Note this is only for numericals atm\n",
        "y_train_num = np.array(FinParameters_All_Train.loc[:, FinParameters_All_Train.columns == 'IT_Flag'])\n",
        "print('Shape of X (Train): {}'.format(X_train_num.shape))\n",
        "print('Shape of y (Train): {}'.format(y_train_num.shape))\n",
        "print(\"Before OverSampling, counts of label '1' (Training Set): {}\".format(sum(y_train_num==1)))\n",
        "print(\"Before OverSampling, counts of label '0' (Training Set): {}\".format(sum(y_train_num==0)))\n",
        "\n",
        "# Apply resampling technique to training set only:\n",
        "ros_Partial = RandomOverSampler(sampling_strategy=MinClassPortion, random_state=0)\n",
        "X_train_ReOVERpartial_num, y_train_ReOVERpartial_num = ros_Partial.fit_resample(X_train_num, y_train_num.ravel())\n",
        "\n",
        "# Apply SMOTE resampling technique to training set only:\n",
        "#sm_ote = SMOTE(random_state=2)\n",
        "#X_train_ReOVERpartial_num, y_train_ReOVERpartial_num = sm_ote.fit_sample(X_train_num, y_train_num.ravel())\n",
        "\n",
        "print('After OverSampling, the shape of the Training Set Input Attributes: {}'.format(X_train_ReOVERpartial_num.shape))\n",
        "print('After OverSampling, the shape of the Training Set Class Attribute: {}'.format(y_train_ReOVERpartial_num.shape))\n",
        "\n",
        "print(\"After OverSampling, counts of label '1' (Training Set): {}\".format(sum(y_train_ReOVERpartial_num==1)))\n",
        "print(\"After OverSampling, counts of label '0' (Training Set): {}\".format(sum(y_train_ReOVERpartial_num==0)))\n",
        "\n",
        "\n",
        "# Also save resampled X and Y values as dataframe\n",
        "df_rand_Train1_num_ReOVERpartial = pd.DataFrame(X_train_ReOVERpartial_num,columns=ParamColumnHeaders[2:]).astype(\"float64\")\n",
        "df_rand_Train1_num_ReOVERpartial[\"IT_Flag\"] = y_train_ReOVERpartial_num.astype(\"float64\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhtIyUlZrMgf",
        "colab_type": "text"
      },
      "source": [
        "***Train Neural Network and Report Performance***\n",
        "<br>\n",
        "<br>This section of the script trains a neural network with various iteration limit cut-offs. Results are outputed into a table for a variety of important metrics. Confusion matrixes are then plotted for cross-validation and test sample performance.\n",
        "\n",
        "<br>It is possible to switch between ADAM and SGD by changing the \"solver\" section of the MLPClassifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaoP8aYgrNRc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "y_train = np.array(df_rand_Train1_num_ReOVERpartial.copy()).astype(\"float64\")\n",
        "X_train = y_train[:,3:-1] # Independent Delta Variables\n",
        "y_train = y_train[:,-1].reshape(-1,1) # Prosecuted IT Case during window\n",
        "\n",
        "y_CV = np.array(FinParameters_All_CV[ParamColumnHeaders].copy()).astype(\"float64\")\n",
        "X_CV = y_CV[:,5:] # Independent Delta Variables\n",
        "y_CV = y_CV[:,0] # Prosecuted IT Case during window\n",
        "\n",
        "y_Test = np.array(FinParameters_All_Test[ParamColumnHeaders].copy()).astype(\"float64\")\n",
        "X_Test = y_Test[:,5:] # Independent Delta Variables\n",
        "y_Test = y_Test[:,0] # Prosecuted IT Case during window\n",
        "\n",
        "# Create Placeholder for Results\n",
        "Res_Row_Lab = [\"Iteration Limit\", \"Run Time\", \"Training Set: Accuracy\", \"Cross-Validation Set: Accuracy\", \"Training Set: F1 Score\", \"Cross-Validation Set: F1 Score\"]\n",
        "NN_Results = pd.DataFrame(np.zeros((len(Res_Row_Lab),1)),columns=[\"Row_Labels\"])\n",
        "NN_Results[\"Row_Labels\"] = Res_Row_Lab\n",
        "\n",
        "\n",
        "Iteration_Limits = [1,5,20,50,100,250]#,500,1000,2000]\n",
        "\n",
        "\n",
        "for Iteration_Limit in Iteration_Limits:\n",
        "    # Start Loop Timer\n",
        "    Loop_Start = tm.time()\n",
        "    # Specify Classifier             \n",
        "    clf_NN = MLPClassifier(hidden_layer_sizes=(5,5), max_iter=Iteration_Limit, alpha=1e-4,\n",
        "                    solver='adam', verbose=10, tol=1e-4, random_state=1,\n",
        "                    learning_rate_init=.1)\n",
        "    #Train the Data\n",
        "    clf_NN.fit(X_train, y_train.ravel())    \n",
        "    # Save output of training data for debugging\n",
        "    Train_Predictions_NN = clf_NN.predict(X_train).reshape(-1,1).astype(\"float64\")\n",
        "    print(\"Training IT Predictions: \",Train_Predictions_NN.sum()/len(Train_Predictions_NN),\"%\")\n",
        "    # Try using the trained NN for predictions:\n",
        "    CrossV_Predictions_NN = clf_NN.predict(X_CV)\n",
        "    CrossV_Predictions_NN = CrossV_Predictions_NN.reshape(-1,1).astype(\"float64\")\n",
        "    Test_Predictions_NN = clf_NN.predict(X_Test).reshape(-1,1).astype(\"float64\")\n",
        "    Temp_F1_Cross = f1_score(y_CV, CrossV_Predictions_NN) \n",
        "    # Mark Loop End Time\n",
        "    LoopRunTime = tm.time() - Loop_Start\n",
        "    print(\"Loop: \",Iteration_Limit,\"\\n\",\"Iteration Time: \",LoopRunTime)\n",
        "    # Save Results\n",
        "    NN_Results[Iteration_Limit] = [Iteration_Limit, LoopRunTime, clf_NN.score(X_train, y_train), clf_NN.score(X_CV, y_CV), F1_Score(confusion_matrix(clf_NN.predict(X_train), y_train)),F1_Score(confusion_matrix(clf_NN.predict(X_CV), y_CV))]\n",
        "\n",
        "clf_NN.score(X_Test, y_Test)\n",
        "F1_Score(confusion_matrix(clf_NN.predict(X_Test), y_Test))\n",
        "\n",
        "\n",
        "NN_ConfuMat = confusion_matrix(y_CV, CrossV_Predictions_NN)\n",
        "print(NN_ConfuMat)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(y_CV, CrossV_Predictions_NN,\n",
        "                      title='NN CV Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(y_CV, CrossV_Predictions_NN, normalize=True,\n",
        "                      title='NN CV Normalized confusion matrix')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(y_Test, Test_Predictions_NN,\n",
        "                      title='NN Testing Set Confusion matrix, without normalization')\n",
        "\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix(y_Test, Test_Predictions_NN, normalize=True,\n",
        "                      title='NN Testing Set Normalized confusion matrix')\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Full Sample\n",
        "print(len(Full_Sample[\"IT_Flag\"]))\n",
        "# % that represent insider trading periods\n",
        "print(100*Full_Sample[\"IT_Flag\"].sum()/len(Full_Sample[\"IT_Flag\"]),\"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}